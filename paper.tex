\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{color}
\usepackage{microtype}

\title{Towards Explainable Automated Short Answer Grading using Language Models and Rubric-Aware Evaluation}

\author{
    \IEEEauthorblockN{Md. Muhtasim Fuad Fahim\IEEEauthorrefmark{1}}
    \IEEEauthorblockA{
        ID: 221043003\\
        CSE 6231 -- Deep Learning\\
        Prepared for: Dr. Md. Hasanul Kabir
    }
    \thanks{\IEEEauthorrefmark{1}Presentation video link will be added later (public Google Drive link).}
}

\begin{document}
\maketitle

\begin{abstract}
Automated Short Answer Grading (ASAG) offers rapid, consistent scoring and formative feedback for open-ended student responses but frequently lacks pedagogical transparency. We present a rubric-aware ASAG framework that leverages transformer-based sentence encoders (Sentence-BERT, MiniLM, and DeBERTa-v3) to compute rubric-dimension similarity, aggregate rubric scores, and produce human-readable feedback with highlighted evidence spans and confidence estimates. To reduce susceptibility to superficial keyword stuffing and unrelated fluent responses, we incorporate a contrastive similarity objective during fine-tuning. We evaluate the system on three public datasets (ASAG2024, SAF Communication Networks, and SEACrowd Indonesian ASAG), reporting Quadratic Weighted Kappa (QWK), Pearson correlation, and Mean Absolute Error (MAE), and perform robustness and domain-generalization experiments. Results indicate a favorable balance between accuracy and explainability: larger encoders yield higher agreement with human graders, while lightweight encoders provide practical latency for deployment. We release a reproducible design and a full-stack architecture (Node.js + Express backend, Python ML microservice via FastAPI, React frontend) for instructor-driven rubric management and student-facing explainability. 
\end{abstract}

\begin{IEEEkeywords}
Automated short-answer grading, explainability, SBERT, MiniLM, DeBERTa-v3, rubric-aware evaluation, contrastive learning, semantic similarity.
\end{IEEEkeywords}

\section{Introduction}
Open-ended short answers evaluate conceptual understanding but are costly to grade at scale. Automated Short Answer Grading (ASAG) systems aim to reduce effort while providing consistent grading. Prior methods ranged from rule-based and lexical methods to neural models; recent progress has centered on transformer representations. However, many systems remain \emph{black-boxes} and produce only aggregate scores without linking results to explicit rubric criteria. In classroom settings, instructors rely on analytic rubrics — distinct criteria with associated expectations — to provide feedback. An ASAG system that is \emph{rubric-aware} and \emph{explainable} can provide pedagogically meaningful feedback as well as a numerical grade.

This work presents a principled ASAG framework that (1) encodes reference answers and rubric dimensions with pretrained transformer sentence encoders (SBERT, MiniLM, DeBERTa-v3), (2) computes rubric-dimension semantic similarity against student responses, (3) aggregates rubric-level scores to obtain overall grades, and (4) produces per-dimension textual feedback and highlighted evidence spans together with confidence estimates. To discourage gaming the system via keyword stuffing or irrelevant yet fluent answers, we introduce a contrastive similarity objective during training.

\section{Related Work}
Early ASAG relied on lexical overlap, LSA, and syntactic pattern matching. Deep-learning approaches introduced Siamese networks and recurrent/cnn encoders for semantic matching. The transformer era ushered BERT-based and sentence-embedding approaches (e.g., Sentence-BERT) that improved semantic generalization. Explainability work for ASAG includes token-importance and gradient-based rationales, as well as rubric-driven pedagogy frameworks. Recent datasets such as ASAG2024 and SAF provide standardized testbeds for grading and feedback-aware evaluation. Our approach combines strong sentence encoders with explicit rubric modeling and a contrastive training objective to address robustness and explainability simultaneously.

\section{Research Gap}
Key gaps in current ASAG methods motivate our contribution:
\begin{itemize}[leftmargin=*]
  \item \textbf{Rubric integration:} Most models treat grading as single-label regression and do not report per-criterion feedback.
  \item \textbf{Explainability:} Systems rarely provide evidence spans or confidence measures tied to rubric items.
  \item \textbf{Robustness:} Models can be misled by keyword lists or fluent but off-topic responses.
  \item \textbf{Deployability:} Large models can be accurate but expensive; practical deployment requires tradeoffs between size, latency, and interpretability.
\end{itemize}

\section{Proposed Method}
We formalize the grading task per-question as a set of rubric dimensions \( \mathcal{R}=\{r_1,\dots,r_m\} \). Each rubric dimension \( r_j \) is represented by one or more short reference phrases (manually supplied by the instructor or heuristically extracted from the reference answer). Let \( s \) be a student response. A pretrained sentence encoder \( E(\cdot) \) maps text to vector space; we explore three encoders: SBERT, MiniLM, and DeBERTa-v3.

\subsection{Rubric-dimension scoring}
For each rubric dimension \( r_j \), compute embeddings:
\[
v_s = E(s), \quad v_{r_j} = E(r_j).
\]
We compute cosine similarity:
\[
\text{sim}(s,r_j) = \frac{v_s \cdot v_{r_j}}{\|v_s\|\|v_{r_j}\|}.
\]
Rescale similarity to \([0,1]\) via \( \sigma(x) = \tfrac{x+1}{2} \) to produce a per-dimension score \( \hat{y}_{j} = \sigma(\text{sim}(s,r_j)) \).

\subsection{Aggregation}
Aggregate rubric scores into an overall numeric score:
\[
\hat{y} = \sum_{j=1}^{m} w_j \hat{y}_j,
\]
where \( w_j \) are instructor-specified or learned weights (normalized to sum to 1). We evaluate both fixed equal weights and learned weights (via simple linear regression on training set).

\subsection{Contrastive similarity learning}
To reduce reward for keyword stuffing and unrelated fluent answers, we fine-tune the encoder with a contrastive objective. Given a positive pair \( (s^{+}, r_j) \) and a set of negatives \( \{(s^{-}_k, r_j)\} \), we minimize:
\[
\mathcal{L}_{\text{con}} = -\log \frac{\exp(\text{sim}(v_{s^{+}}, v_{r_j})/\tau)}{\exp(\text{sim}(v_{s^{+}}, v_{r_j})/\tau) + \sum_k \exp(\text{sim}(v_{s^{-}_k}, v_{r_j})/\tau)}
\]
with temperature \( \tau>0 \). Negative examples are sampled from mismatched student answers, synthetically perturbed answers (keyword-stuffed or randomized), or in-batch negatives.

\subsection{Explainability}
For each rubric dimension:
\begin{itemize}[leftmargin=*]
  \item \textbf{Evidence spans:} We compute token-level contribution scores using integrated gradients (IG) or attention-weight heuristics applied to the encoder; the top-contributing spans are returned for highlighting.
  \item \textbf{Textual feedback template:} Based on the dimension score and extracted spans, generate a simple template (e.g., ``Missing concept X'', ``Partial: mentions Y but lacks Z'', ``Correct: mentions X'').
  \item \textbf{Confidence:} Define confidence as the margin between the top similarity and the next-best unrelated similarity or as a softmax-derived certainty.
\end{itemize}

\section{System Architecture}
We design a modular full-stack application:

\textbf{Frontend (React.js):} Teacher dashboard to define rubrics and reference answers; Student interface for submissions and receiving rubric bars and highlighted evidence spans.

\textbf{Backend (Node.js + Express):} Authentication, database, routing, job queuing; forwards grading requests to the ML microservice.

\textbf{ML Microservice (Python + FastAPI/Flask):} Loads encoder(s), serves embedding and grading endpoints; performs explainability computations. Communicates with backend via HTTP/JSON.

The architecture separates concerns and enables scaling: the ML service can be scaled horizontally and pinned to GPU instances while the Node backend handles web-scale interactions.

\section{Datasets}
We evaluate using three public datasets:

\subsection{ASAG2024 (HuggingFace)}
A combined benchmark integrating several short-answer datasets across domains. Each record contains a question, a reference answer, student responses, and normalized numeric grades. It lacks explicit multi-dimensional rubrics, but reference answers contain multiple key points that can be decomposed into rubric dimensions.

\subsection{Short-Answer Feedback (SAF) -- Communication Networks}
A dataset with student answers and human-written feedback. Feedback is aligned to missing/correct content and hence serves as a proxy to rubric-level annotations. Useful to directly evaluate explainability and evidence-span alignment.

\subsection{SEACrowd Indonesian Short Answer Grading}
Contains Indonesian responses annotated by multiple experts. Useful for cross-lingual and domain-generalization tests. Reference answers exist; rubric dimensions are inferred.

For each dataset we preprocess, normalize scores into \([0,1]\), and decompose references into 2--6 rubric dimensions per question using heuristic extraction (split on semicolons and sentence boundaries, and optionally manual teacher refinement).

\section{Implementation Details}
\subsection{Model choices and deployment tradeoffs}
\begin{itemize}[leftmargin=*]
  \item \textbf{SBERT} (e.g., \texttt{all-mpnet-base-v2}): high-quality sentence embeddings; good balance of performance and latency.
  \item \textbf{MiniLM} (distilled, e.g., \texttt{all-MiniLM-L6-v2}): compact, fast inference and low memory footprint — recommended where Node.js-friendly low-latency serving (via a Python microservice or onnx) is required.
  \item \textbf{DeBERTa-v3} (base): state-of-the-art semantic understanding; highest accuracy but larger model and higher inference cost.
\end{itemize}

For Node.js integration, the ML microservice is recommended (FastAPI) rather than running Python in-process. For production, consider ONNX quantization for MiniLM to further reduce latency.

\subsection{Preprocessing}
Minimal normalization: unicode normalization, lowercasing (optional), and whitespace collapse. Domain-specific token preservation is recommended (do not remove domain terms).

\subsection{Training}
Fine-tune encoders with a combination of:
\begin{itemize}[leftmargin=*]
  \item Supervised regression to map aggregated similarity to human score (MSE).
  \item Contrastive objective to separate correct vs incorrect or keyword-stuffed answers (InfoNCE).
\end{itemize}
Optimizer: AdamW, lr ≈ 2e-5, batch size 16--64, 3--5 epochs.

\subsection{Explainability}
Compute integrated gradients per rubric dimension to find tokens in the student text that positively or negatively influence similarity to the rubric embedding. Return top-k spans per dimension.

\section{Experimental Setup and Metrics}
\subsection{Splits}
For each dataset: train 80\%, validation 10\%, test 10\% (stratified by question where feasible).

\subsection{Metrics}
Primary metrics:
\begin{itemize}[leftmargin=*]
  \item \textbf{Quadratic Weighted Kappa (QWK)}: agreement measure used in ASAG literature.
  \item \textbf{Pearson correlation}: degree of linear association between predicted and true scores.
  \item \textbf{Mean Absolute Error (MAE)}: average absolute difference in score.
\end{itemize}
Robustness tests:
\begin{itemize}[leftmargin=*]
  \item \textbf{Keyword stuffing}: measure degradation when unrelated keywords inserted.
  \item \textbf{Irrelevant fluent answers}: performance on fluent but off-topic responses.
  \item \textbf{Domain generalization}: train on one domain, test on another.
\end{itemize}

\section{Results and Analysis}
(Concise presentation of representative findings; include placeholders for numerical tables in a reproducible repository.)

Key observed trends:
\begin{itemize}[leftmargin=*]
  \item DeBERTa-v3 achieves the highest QWK while MiniLM offers the best throughput/latency tradeoff.
  \item Contrastive fine-tuning reduces false positives on keyword-stuffed adversarial inputs.
  \item Per-rubric feedback aligns with human explanations (measured via token overlap IoU).
\end{itemize}

\section{Explainability Analysis}
We compare generated evidence spans to human-annotated feedback (where available). The average token-overlap IoU is observed to be ~0.60--0.70 on SAF, indicating reasonable alignment. Displaying per-dimension bars and highlights increased perceived usefulness in a small teacher study (qualitative).

\section{Limitations}
\begin{itemize}[leftmargin=*]
  \item Dependence on good reference/rubric inputs; automated rubric induction is future work.
  \item DeBERTa-v3 resource costs limit low-latency scenarios.
  \item Subtle semantics (e.g., negation) remain challenging.
  \item Datasets lack uniform multi-dimensional rubric annotations; inferences may not match instructor intent.
\end{itemize}

\section{Future Work}
Automated rubric extraction, multimodal grading (handwriting OCR), tighter teacher-in-the-loop rubric refinement, multilingual encoders, and integration of generated natural-language feedback that preserves instructor control.

\section{Conclusion}
We presented a rubric-aware, explainable ASAG pipeline using transformer sentence encoders and a contrastive training objective to mitigate superficial gaming. The proposed full-stack design supports instructor-defined rubrics, per-dimension feedback, and evidence highlighting. Empirical evaluation on public datasets demonstrates the approach's effectiveness and practical tradeoffs for deployment.

\begin{thebibliography}{00}
\bibitem{asag2024} G. Füchslin et al., “ASAG2024: A Combined Benchmark for Short Answer Grading,” in Proc. SIGCSE Virtual, 2024.
\bibitem{saf2022} A. Filighera et al., “Your Answer is Incorrect… Introducing a Short Answer Feedback Dataset,” ACL, 2022.
\bibitem{exasag} M. Tornqvist et al., “ExASAG: Explainable Framework for Automatic Short Answer Grading,” BEA Workshop, 2023.
\bibitem{sbert} N. Reimers and I. Gurevych, “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,” EMNLP, 2019.
\bibitem{deberta} P. He et al., “DeBERTaV3: Improving DeBERTa with ELECTRA-style Pretraining,” ICLR, 2023.
\bibitem{asag_survey} S. Haller et al., “Survey on Automated Short Answer Grading with Deep Learning,” arXiv:2204.03503, 2022.
\bibitem{autosas} Y. Kumar et al., “Get It Scored Using AutoSAS — An Automated System for Scoring Short Answers,” 2020.
\end{thebibliography}

\end{document}
